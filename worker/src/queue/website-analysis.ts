import { Queue, Worker } from 'bullmq'
import { createClient } from '@supabase/supabase-js'
import { WebsiteCrawler } from '../analyzers/website-crawler.js'
import { RecommendationEngine, Recommendation } from '../analyzers/recommendation-engine.js'
import { AIRecommendationEngine, AIRecommendation } from '../analyzers/ai-recommendation-engine.js'
import { PromptGenerator } from '../analyzers/prompt-generator.js'
import { PageDiscovery, DiscoveredPage, PageContent } from '../analyzers/page-discovery.js'
import { sendScanCompletedEmail } from '../services/email.js'
import { createRedisConnection } from '../utils/redis.js'

const redisConnection = createRedisConnection()

const supabase = createClient(
  process.env.SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_ROLE_KEY || process.env.SUPABASE_SERVICE_KEY!
)

export interface WebsiteAnalysisJobData {
  organizationId: string
  productId?: string
  domain: string
  includeCompetitorGaps?: boolean
  multiPageAnalysis?: boolean // New option for multi-page crawling
  jobId?: string
}

/**
 * Website Analysis Queue
 * Crawls website, generates recommendations, stores in database
 */
export const websiteAnalysisQueue = new Queue<WebsiteAnalysisJobData>('website-analysis', {
  connection: redisConnection,
  defaultJobOptions: {
    attempts: 3,
    backoff: {
      type: 'exponential',
      delay: 5000
    },
    removeOnComplete: 100,
    removeOnFail: 100
  }
})

/**
 * Website Analysis Worker
 * Processes website analysis jobs
 */
export const websiteAnalysisWorker = new Worker<WebsiteAnalysisJobData>(
  'website-analysis',
  async (job) => {
    const { organizationId, productId, domain, includeCompetitorGaps = false, multiPageAnalysis = true } = job.data

    console.log(`[Website Analysis] Starting analysis for ${domain} (productId: ${productId || 'none'}, multiPage: ${multiPageAnalysis})`)

    try {
      // 0. Validate organization exists before doing any work
      const { data: org, error: orgError } = await supabase
        .from('organizations')
        .select('id')
        .eq('id', organizationId)
        .single()

      if (orgError || !org) {
        console.error(`[Website Analysis] Organization ${organizationId} not found, skipping job`)
        return { skipped: true, reason: 'Organization not found' }
      }

      // 1. Crawl and analyze main website page
      console.log(`[Website Analysis] Crawling main website...`)
      const crawler = new WebsiteCrawler()
      const websiteAnalysis = await crawler.analyze(domain)

      // 2. Store website analysis results
      console.log(`[Website Analysis] Storing website analysis...`)
      const { data: analysisRecord, error: analysisError } = await supabase
        .from('website_analyses')
        .insert({
          organization_id: organizationId,
          product_id: productId || null,
          domain,
          tech_stack: websiteAnalysis.techStack,
          schema_markup: websiteAnalysis.schemaMarkup,
          content_structure: websiteAnalysis.contentStructure,
          technical_seo: websiteAnalysis.technicalSEO,
          aeo_readiness: websiteAnalysis.aeoReadiness,
          analyzed_at: websiteAnalysis.analyzedAt
        })
        .select()
        .single()

      if (analysisError) {
        console.error('[Website Analysis] Error storing analysis:', analysisError)
        throw analysisError
      }

      // 3. Multi-page discovery and analysis
      let discoveredPages: DiscoveredPage[] = []
      let pageContents: PageContent[] = []
      // Page recommendations are now generated by AI, not rule-based

      if (multiPageAnalysis) {
        console.log(`[Website Analysis] Discovering pages...`)
        const pageDiscovery = new PageDiscovery()
        discoveredPages = await pageDiscovery.discoverPages(domain)

        // Store discovered pages
        const pagesToStore = discoveredPages.map(page => ({
          organization_id: organizationId,
          product_id: productId || null,
          url: page.url,
          path: page.path,
          title: page.title,
          is_relevant: page.isRelevant,
          relevance_reason: page.relevanceReason,
          content_type: page.contentType,
          last_crawled_at: new Date().toISOString()
        }))

        if (pagesToStore.length > 0) {
          // Upsert crawled pages (update if exists)
          await supabase
            .from('crawled_pages')
            .upsert(pagesToStore, {
              onConflict: 'organization_id,url',
              ignoreDuplicates: false
            })
        }

        // Fetch content from relevant pages
        const relevantPages = discoveredPages.filter(p => p.isRelevant).slice(0, 20) // Limit to 20 pages
        console.log(`[Website Analysis] Fetching content from ${relevantPages.length} relevant pages...`)

        pageContents = await pageDiscovery.fetchPageContents(relevantPages)

        // Analyze each page first to get analysis data
        console.log(`[Website Analysis] Analyzing page content...`)
        const pageAnalyses: Array<{
          url: string
          title: string
          contentType: string
          content: string
          analysis: any
        }> = []

        for (const pageContent of pageContents) {
          const pageAnalysis = await crawler.analyzeHtml(pageContent.html, pageContent.url)

          pageAnalyses.push({
            url: pageContent.url,
            title: pageContent.title,
            contentType: pageContent.contentType,
            content: pageContent.textContent,
            analysis: pageAnalysis
          })

          // Update crawled_pages with analysis data
          await supabase
            .from('crawled_pages')
            .update({
              aeo_score: pageAnalysis.aeoReadiness.score,
              word_count: pageAnalysis.contentStructure.wordCount,
              has_schema: pageAnalysis.schemaMarkup.length > 0,
              title: pageContent.title
            })
            .eq('organization_id', organizationId)
            .eq('url', pageContent.url)
        }

        // Page-specific recommendations will be generated by AI after product analysis
        // Store pageAnalyses for later use
        pageContents = pageContents.map((pc, i) => ({
          ...pc,
          analysis: pageAnalyses[i]?.analysis
        })) as any
      }

      // 4. Generate prompts using AI
      console.log(`[Website Analysis] Generating prompts...`)
      const promptGenerator = new PromptGenerator()

      // First, analyze the product/service
      const productAnalysis = await promptGenerator.analyzeProduct(
        websiteAnalysis,
        websiteAnalysis.textContent
      )

      console.log(`[Website Analysis] Product identified: ${productAnalysis.productName}`)

      // Store product analysis
      await supabase
        .from('product_analyses')
        .upsert({
          organization_id: organizationId,
          product_id: productId || null,
          domain,
          product_name: productAnalysis.productName,
          product_description: productAnalysis.productDescription,
          key_features: productAnalysis.keyFeatures,
          target_audience: productAnalysis.targetAudience,
          use_cases: productAnalysis.useCases,
          differentiators: productAnalysis.differentiators,
          analyzed_at: new Date().toISOString()
        }, {
          onConflict: productId ? 'product_id' : 'organization_id'
        })

      // Generate prompts (5 topics Ã— 3 granularity levels = 15 prompts)
      const generatedPrompts = await promptGenerator.generatePrompts(
        productAnalysis,
        websiteAnalysis
      )

      console.log(`[Website Analysis] Generated ${generatedPrompts.length} prompts`)

      // Delete existing prompts for this product/organization (to avoid duplicates)
      let deleteQuery = supabase
        .from('prompts')
        .delete()
        .eq('organization_id', organizationId)
        .eq('is_custom', false)

      if (productId) {
        deleteQuery = deleteQuery.eq('product_id', productId)
      }
      await deleteQuery

      // Store generated prompts
      const promptsToInsert = generatedPrompts.map(p => ({
        organization_id: organizationId,
        product_id: productId || null,
        prompt_text: p.promptText,
        category: p.category,
        granularity_level: p.granularityLevel,
        is_custom: false
      }))

      const { error: promptsError } = await supabase
        .from('prompts')
        .insert(promptsToInsert)

      if (promptsError) {
        console.error('[Website Analysis] Error storing prompts:', promptsError)
        // Don't throw - prompts are not critical for completion
      } else {
        console.log(`[Website Analysis] Stored ${promptsToInsert.length} prompts`)
      }

      // 5. Get recent scan results for context
      console.log(`[Website Analysis] Fetching recent scan results...`)
      const { data: scanResults } = await supabase
        .from('prompt_results')
        .select('*')
        .eq('organization_id', organizationId)
        .order('tested_at', { ascending: false })
        .limit(50)

      // 6. Get competitor gaps if requested
      let competitorGaps: any[] = []
      if (includeCompetitorGaps) {
        console.log(`[Website Analysis] Fetching competitor gaps...`)
        const { data: gaps } = await supabase
          .from('visibility_gaps')
          .select('*, prompts(prompt_text), competitors(name)')
          .eq('organization_id', organizationId)
          .is('resolved_at', null)
          .limit(10)

        competitorGaps = gaps || []
      }

      // 7. Generate AI-powered recommendations using research knowledge base
      console.log(`[Website Analysis] Generating AI-powered recommendations...`)
      const aiRecommendationEngine = new AIRecommendationEngine()

      let aiRecommendations: AIRecommendation[] = []
      try {
        aiRecommendations = await aiRecommendationEngine.generateRecommendations(
          websiteAnalysis,
          scanResults || [],
          {
            productName: productAnalysis.productName,
            productDescription: productAnalysis.productDescription,
            targetAudience: productAnalysis.targetAudience
          },
          { maxRecommendations: 10 } // Reduced since we'll add page-specific ones
        )
        console.log(`[Website Analysis] Generated ${aiRecommendations.length} general AI recommendations`)
      } catch (error) {
        console.error('[Website Analysis] AI recommendation generation failed:', error)
      }

      // 8. Generate AI-powered page-specific recommendations (replaces rule-based)
      let aiPageRecommendations = new Map<string, AIRecommendation[]>()
      if (pageContents.length > 0) {
        console.log(`[Website Analysis] Generating AI page-specific recommendations for ${pageContents.length} pages...`)
        try {
          // Prepare page data for AI
          const pagesForAI = pageContents.map((pc: any) => ({
            url: pc.url,
            title: pc.title,
            contentType: pc.contentType,
            content: pc.textContent || '',
            analysis: pc.analysis || websiteAnalysis
          }))

          // Get existing recommendation titles to avoid duplicates
          const existingTitles = aiRecommendations.map(r => r.title)

          aiPageRecommendations = await aiRecommendationEngine.generatePageRecommendations(
            pagesForAI,
            {
              productName: productAnalysis.productName,
              productDescription: productAnalysis.productDescription
            },
            existingTitles
          )
          console.log(`[Website Analysis] Generated AI recommendations for ${aiPageRecommendations.size} pages`)
        } catch (error) {
          console.error('[Website Analysis] AI page recommendations failed:', error)
        }
      }

      // Fallback to rule-based recommendations ONLY if AI completely fails
      const recommendationEngine = new RecommendationEngine()
      let fallbackRecommendations: Recommendation[] = []

      if (aiRecommendations.length === 0 && aiPageRecommendations.size === 0) {
        console.log(`[Website Analysis] Using rule-based recommendations as fallback`)
        fallbackRecommendations = recommendationEngine.generateRecommendations(
          websiteAnalysis,
          scanResults || [],
          competitorGaps
        )
      }

      // 9. Prepare all recommendations for storage
      console.log(`[Website Analysis] Storing recommendations...`)

      // Delete existing pending recommendations to avoid duplicates
      let deleteRecsQuery = supabase
        .from('fix_recommendations')
        .delete()
        .eq('organization_id', organizationId)
        .eq('status', 'pending')

      if (productId) {
        deleteRecsQuery = deleteRecsQuery.eq('product_id', productId)
      }
      await deleteRecsQuery

      const allRecommendationsToInsert: any[] = []

      // Add AI-powered general recommendations (primary source)
      for (const rec of aiRecommendations) {
        allRecommendationsToInsert.push({
          organization_id: organizationId,
          product_id: productId || null,
          title: rec.title,
          description: rec.description,
          category: rec.category,
          priority: rec.priority,
          estimated_impact: rec.estimatedImpact,
          implementation_guide: rec.implementationGuide,
          code_snippets: rec.codeSnippets || [],
          estimated_time: rec.estimatedTime,
          difficulty: rec.difficulty,
          status: 'pending',
          page_url: null,
          page_title: rec.aiPlatformSpecific?.length
            ? `Optimized for: ${rec.aiPlatformSpecific.join(', ')}`
            : 'General',
          ai_generated: true,
          research_insight: rec.researchInsight || null
        })
      }

      // Add AI-powered page-specific recommendations
      for (const [pageUrl, recs] of aiPageRecommendations) {
        const pageContent = pageContents.find((p: any) => p.url === pageUrl)
        const pageTitle = pageContent?.title || new URL(pageUrl).pathname

        for (const rec of recs) {
          allRecommendationsToInsert.push({
            organization_id: organizationId,
            product_id: productId || null,
            title: rec.title,
            description: rec.description,
            category: rec.category,
            priority: rec.priority,
            estimated_impact: rec.estimatedImpact,
            implementation_guide: rec.implementationGuide,
            code_snippets: rec.codeSnippets || [],
            estimated_time: rec.estimatedTime,
            difficulty: rec.difficulty,
            status: 'pending',
            page_url: pageUrl,
            page_title: pageTitle,
            ai_generated: true,
            research_insight: rec.researchInsight || null
          })
        }
      }

      // Add rule-based fallback recommendations (only if AI completely failed)
      for (const rec of fallbackRecommendations) {
        allRecommendationsToInsert.push({
          organization_id: organizationId,
          product_id: productId || null,
          title: rec.title,
          description: rec.description,
          category: rec.category,
          priority: rec.priority,
          estimated_impact: rec.estimatedImpact,
          implementation_guide: rec.implementationGuide,
          code_snippets: rec.codeSnippets || [],
          estimated_time: rec.estimatedTime,
          difficulty: rec.difficulty,
          status: 'pending',
          page_url: null,
          page_title: 'Homepage',
          ai_generated: false
        })
      }

      const { error: recsError } = await supabase
        .from('fix_recommendations')
        .insert(allRecommendationsToInsert)

      if (recsError) {
        console.error('[Website Analysis] Error storing recommendations:', recsError)
        throw recsError
      }

      const totalRecommendations = allRecommendationsToInsert.length
      const pagesAnalyzed = pageContents.length + 1 // +1 for homepage

      console.log(`[Website Analysis] Successfully completed analysis for ${domain}`)
      console.log(`[Website Analysis] - Pages analyzed: ${pagesAnalyzed}`)
      console.log(`[Website Analysis] - Recommendations generated: ${totalRecommendations}`)

      // Mark job as completed if jobId provided
      if (job.data.jobId) {
        await supabase
          .from('jobs')
          .update({
            status: 'completed',
            completed_at: new Date().toISOString()
          })
          .eq('id', job.data.jobId)
      }

      // Update product's aeo_score and last_analyzed_at if productId provided
      if (productId) {
        await supabase
          .from('products')
          .update({
            aeo_score: websiteAnalysis.aeoReadiness.score,
            last_analyzed_at: new Date().toISOString()
          })
          .eq('id', productId)
      }

      return {
        success: true,
        domain,
        productId,
        aeoReadiness: websiteAnalysis.aeoReadiness.score,
        recommendationsCount: totalRecommendations,
        pagesAnalyzed,
        analysisId: analysisRecord.id
      }
    } catch (error) {
      console.error('[Website Analysis] Error:', error)

      // Mark job as failed if jobId provided
      if (job.data.jobId) {
        await supabase
          .from('jobs')
          .update({
            status: 'failed',
            completed_at: new Date().toISOString(),
            error_message: error instanceof Error ? error.message : 'Unknown error'
          })
          .eq('id', job.data.jobId)
      }

      throw error
    }
  },
  {
    connection: redisConnection,
    concurrency: 2 // Process 2 analyses at a time
  }
)

// Event listeners
websiteAnalysisWorker.on('completed', async (job, result) => {
  console.log(`[Website Analysis] Job ${job.id} completed successfully`)

  try {
    // Get organization and owner for email notification
    const { data: org } = await supabase
      .from('organizations')
      .select('id, name')
      .eq('id', job.data.organizationId)
      .single()

    if (!org) return

    const { data: owner } = await supabase
      .from('profiles')
      .select('email, full_name')
      .eq('organization_id', org.id)
      .eq('role', 'owner')
      .single()

    if (!owner?.email) return

    // Send completion email
    await sendScanCompletedEmail({
      recipientEmail: owner.email,
      recipientName: owner.full_name || 'there',
      brandName: org.name,
      scanType: 'website',
      totalScans: result.recommendationsCount,
      dashboardUrl: `${process.env.APP_URL || 'https://columbus-aeo.com'}/dashboard/recommendations`
    })

    console.log(`[Website Analysis] Notification email sent to ${owner.email}`)
  } catch (error) {
    console.error('[Website Analysis] Error sending completion email:', error)
    // Don't throw - email failure shouldn't affect job completion
  }
})

websiteAnalysisWorker.on('failed', (job, err) => {
  console.error(`[Website Analysis] Job ${job?.id} failed:`, err.message)
})

websiteAnalysisWorker.on('error', (err) => {
  console.error('[Website Analysis] Worker error:', err)
})

console.log('[Website Analysis] Worker started')
